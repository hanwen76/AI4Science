## 强化学习 in AI4S
- 发现任意矩阵乘法的有效且可证明正确的算法
- 自动机器人马斯克Optimus
## Deep RL = RL + NN
- difference between SL and RL
  - 从标注数据学习 vs 从经验中学习
  - 开环 vs 闭环，有反馈
  - 预先给定的数据"by examples" vs 探索环境产生的数据"by experience"
- why?
  - model-free decision 不需要对任务进行形式化解析
  - 算法应用范围广：“无需了解汽车的驾驶原理，却可以超过人类的驾驶水平”
## 马尔可夫决策MDP
- 过程：给定当前状态 -> 选择动作 -> 转移到下一状态
- RL旨在求解：在MDP中如何选择动作
- Description
  - 状态值函数：从当前状态s开始的期望奖励 -> 进一步分解为及时奖励和下一个状态值
  - Return：从t时刻开始的累计奖励
- 几种function
  - 策略函数π(s)（输出每个动作执行的概率/值）
  - 状态价值函数V(s)（从状态st开始，都按照策略π(s)与环境交互，输出从st+1到 s∞ 过程中得到的reward累加的期望）
  - 状态动作价值函数Q(s,a)（给定一个状态st，采取动作at后，从st+1 开始按照某一策略π(s)与环境继续进行交互，输出得到的累计回报的期望值）
- How to take action? “value Learning” -> Find Q(s,a) then a = argmax Q(s,a)
  - 在每一个状态上，依据最优Q函数，采取最优的动作a，得到的价值Q*(s,a)
- Bellman Equation
  - 最优价值函数𝑉∗ (s) 和最优Q函数𝑄∗ (𝑠, 𝑎)可以利用BE互相转化
- 值函数（Q函数）更新
  - 蒙特卡洛方法（回合更新）
  - 时间差分学习方法（单步更新）
  - TD(𝜆)
- 动作采样
  - 探索 “以前从来没有做过的事情”
  - 利用 “做当前知道的能产生最大回报的事情，帮助网络训练”
  - 抖动策略
## 深度强化学习 vs 传统强化学习
- 环境状态：有限 vs 无穷尽
- 值函数/Q函数：表格化 vs 深度神经网络
## 强化学习的类型
- 基于学习目标的强化学习分类
  - value-based 输出的是动作价值，选择价值最高动作
  - policy-based 输出执行动作的概率，根据概率来选取动作
  - actor-critic Actor根据概率做出动作，Critic根据动作给出价值
- 基于目标值更新的强化学习分类
  - 回合更新
  - 单步更新
- 根据采样策略的强化学习分类
  - 同策略学习(on-policy) 学习的过程agent必须参与其中
  - 异策略学习(off-policy) 既可以自己参与其中，也可以根据他人学习过程进行学习
- 根据是否借助环境模型的强化学习分类
  - model-based 学会用一个模型来模拟环境，通过模拟的环境来得到反馈
  - model-free 不去学习和理解环境，没有模型，有策略或者值函数
## Q-learning (valued-based)
- 用任何策略来估计Q，以最大化未来回报
- 使用贝尔曼方程更新 Q 函数
- problem：状态空间很大，则更新Q表效率很低 -> DQN
## DQN: Deep RL = RL + Neural Networks
- 输入状态，通过神经网络，为每种动作估计 Q 值!
- Q-learning & Deep Q-learning
  - 给定当前状态和动作，查表得到其Q值，再根据情况更新
  - 给定当前状态，学习得到不同动作及其Q值，因此可根据Q值来选择动作
- Tricks
  - 经验回放 “考前复习” & “复习时遍历不同的错误”
  - Fixed Q-targets 阶段性地固定 Q 目标值，缓解学习的不稳定
- Expend
  - Double DQN
  - 竞争性Dueling DQN
  - 优先经验回放 定义经验优先级
## 策略梯度 (policy-based)
- DQN: 学习值函数，该函数将每个状态动作对 (s,a) 映射到一个值，然后选择价值最大的动作执行
- PG: 直接学习将状态映射到动作的策略函数
## 演员评论家 (actor-critic)
- 合并了以策略为基础的Policy Gradient和以值为基础的Q-Learning:将前者当作Actor，用来基于概率选择行为。将后者当作Critic，用来评判Actor的行为得分，然后Actor又会根据Critic的评分修改行为的概率。
- A2C
## 分布式强化学习框架
- A3C
- IMPALA
- SeedRL
## 多智能体强化学习
- MADDPG
- COMA
- QMIX
  - 采用混合网络对单智能体局部值函数进行合并
  - 训练过程加入全局状态信息
## AlphaGo
- 博弈树：树形结构表示博弈过程
- 遍历博弈树 -> 蒙特卡洛树搜索MCTS
- 策略网络 & 快速策略网络 & 价值网络 共三个网络
## 强化学习现状
- 成也model-free，败也model-free
  - 反馈是稀疏的，没有大量采样支撑
  - 追求通用性儿舍弃了问题本身的知识
- 框架具有通用性，难以从仿真环境迁移到真实世界
- 深度学习和强化学习并重
  - 加强对环境的深度学习
  - 小样本下的拟合能力
